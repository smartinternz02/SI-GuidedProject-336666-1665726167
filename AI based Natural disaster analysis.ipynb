{"cells":[{"cell_type":"markdown","metadata":{"id":"ROJqQBxDn4w5"},"source":["# AI based Natural disaster analysis"]},{"cell_type":"markdown","metadata":{"id":"-H-1hVuHn4xE"},"source":["### Importing Neccessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cveutpg8n4xH"},"outputs":[],"source":["import numpy as np#used for numerical analysis\n","import tensorflow #open source used for both ML and DL for computation\n","from tensorflow.keras.models import Sequential #it is a plain stack of layers\n","from tensorflow.keras import layers #A layer consists of a tensor-in tensor-out computation function\n","#Dense layer is the regular deeply connected neural network layer\n","from tensorflow.keras.layers import Dense,Flatten\n","#Faltten-used fot flattening the input or change the dimension\n","from tensorflow.keras.layers import Conv2D,MaxPooling2D #Convolutional layer\n","#MaxPooling2D-for downsampling the image\n","from keras.preprocessing.image import ImageDataGenerator\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPfqZ_Myn4xL","outputId":"3cd260e4-d1a3-4ea5-ff5e-63804223f402"},"outputs":[{"data":{"text/plain":["'2.8.0'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tensorflow.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ET1-96WPn4xO","outputId":"0af22304-72e3-4f24-d8c2-b3b9da685947"},"outputs":[{"data":{"text/plain":["'2.8.0'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tensorflow.keras.__version__"]},{"cell_type":"markdown","metadata":{"id":"JYIr983kn4xQ"},"source":["### Image Data Agumentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDFsdfJcn4xR"},"outputs":[],"source":["#setting parameter for Image Data agumentation to the training data\n","train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n","#Image Data agumentation to the testing data\n","test_datagen=ImageDataGenerator(rescale=1./255)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gBO4oVNn4xT"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"2gbeL_hnd3De"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CC-nD9j9n4xV"},"source":["### Loading our data and performing data agumentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYhkux8Sn4xX","outputId":"f4f38a49-91ea-4365-d929-2e8ee67afa83"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 742 images belonging to 4 classes.\n","Found 198 images belonging to 4 classes.\n"]}],"source":["#performing data agumentation to train data\n","x_train = train_datagen.flow_from_directory('../dataset/train_set',target_size=(64, 64),batch_size=5,\n","                                            color_mode='rgb',class_mode='categorical')\n","#performing data agumentation to test data\n","x_test = test_datagen.flow_from_directory('../dataset/test_set',target_size=(64, 64),batch_size=5,\n","                                            color_mode='rgb',class_mode='categorical') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BgzzBDLn4xa","outputId":"22308b5b-c7ee-47a0-e8d7-b92178bfe6cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Cyclone': 0, 'Earthquake': 1, 'Flood': 2, 'Wildfire': 3}\n"]}],"source":["print(x_train.class_indices)#checking the number of classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2N9Pw8FZn4xb","outputId":"6628ebe5-0426-41ee-ac2b-bcea4140ef1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Cyclone': 0, 'Earthquake': 1, 'Flood': 2, 'Wildfire': 3}\n"]}],"source":["print(x_test.class_indices)#checking the number of classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6VMOmMhn4xc","outputId":"8958d5d4-bf1e-4229-b1e9-9b7a49b8cfc6"},"outputs":[{"data":{"text/plain":["Counter({0: 220, 1: 156, 2: 198, 3: 168})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from collections import Counter as c\n","c(x_train .labels)"]},{"cell_type":"markdown","metadata":{"id":"Wkg7rklzn4xe"},"source":["### Creating the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rQ5X8D0n4xf"},"outputs":[],"source":["# Initializing the CNN\n","classifier = Sequential()\n","\n","# First convolution layer and pooling\n","classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n","classifier.add(MaxPooling2D(pool_size=(2, 2)))\n","# Second convolution layer and pooling\n","classifier.add(Conv2D(32, (3, 3), activation='relu'))\n","# input_shape is going to be the pooled feature maps from the previous convolution layer\n","classifier.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# Flattening the layers\n","classifier.add(Flatten())\n","\n","# Adding a fully connected layer\n","classifier.add(Dense(units=128, activation='relu'))\n","classifier.add(Dense(units=4, activation='softmax')) # softmax for more than 2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4P5yBKHun4xg","outputId":"6ae7d410-8bd4-48ce-9078-f9b8b808f72d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 62, 62, 32)        896       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 6272)              0         \n","                                                                 \n"," dense (Dense)               (None, 128)               802944    \n","                                                                 \n"," dense_1 (Dense)             (None, 4)                 516       \n","                                                                 \n","=================================================================\n","Total params: 813,604\n","Trainable params: 813,604\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["classifier.summary()#summary of our model"]},{"cell_type":"markdown","metadata":{"id":"f-DVQcyRn4xh"},"source":["### Compiling the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9rYUDFon4xj"},"outputs":[],"source":["# Compiling the CNN\n","# categorical_crossentropy for more than 2\n","classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) "]},{"cell_type":"markdown","metadata":{"id":"tDfgicrln4xk"},"source":["## Fitting the model"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"CWU8iS-Vn4xl","outputId":"d539a3eb-c334-47a7-d936-94e969d3d77e"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_7732/549542485.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  classifier.fit_generator(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","149/149 [==============================] - 136s 876ms/step - loss: 1.1739 - accuracy: 0.4542 - val_loss: 1.1279 - val_accuracy: 0.5253\n","Epoch 2/20\n","149/149 [==============================] - 72s 487ms/step - loss: 0.7935 - accuracy: 0.6685 - val_loss: 0.8564 - val_accuracy: 0.6616\n","Epoch 3/20\n","149/149 [==============================] - 72s 486ms/step - loss: 0.7362 - accuracy: 0.7129 - val_loss: 0.7500 - val_accuracy: 0.7020\n","Epoch 4/20\n","149/149 [==============================] - 73s 490ms/step - loss: 0.6701 - accuracy: 0.7547 - val_loss: 1.3990 - val_accuracy: 0.5152\n","Epoch 5/20\n","149/149 [==============================] - 91s 613ms/step - loss: 0.6437 - accuracy: 0.7493 - val_loss: 0.7660 - val_accuracy: 0.7424\n","Epoch 6/20\n","149/149 [==============================] - 78s 528ms/step - loss: 0.5630 - accuracy: 0.7938 - val_loss: 0.7220 - val_accuracy: 0.7576\n","Epoch 7/20\n","149/149 [==============================] - 95s 639ms/step - loss: 0.5100 - accuracy: 0.8261 - val_loss: 1.4338 - val_accuracy: 0.5253\n","Epoch 8/20\n","149/149 [==============================] - 75s 503ms/step - loss: 0.4424 - accuracy: 0.8261 - val_loss: 0.9343 - val_accuracy: 0.7222\n","Epoch 9/20\n","149/149 [==============================] - 71s 474ms/step - loss: 0.4469 - accuracy: 0.8248 - val_loss: 0.7525 - val_accuracy: 0.7879\n","Epoch 10/20\n","149/149 [==============================] - 74s 500ms/step - loss: 0.4347 - accuracy: 0.8491 - val_loss: 0.6535 - val_accuracy: 0.7677\n","Epoch 11/20\n","149/149 [==============================] - 73s 487ms/step - loss: 0.3709 - accuracy: 0.8747 - val_loss: 0.7740 - val_accuracy: 0.7828\n","Epoch 12/20\n","149/149 [==============================] - 72s 483ms/step - loss: 0.4011 - accuracy: 0.8625 - val_loss: 0.7259 - val_accuracy: 0.7879\n","Epoch 13/20\n","149/149 [==============================] - 72s 485ms/step - loss: 0.3141 - accuracy: 0.8814 - val_loss: 0.8764 - val_accuracy: 0.7273\n","Epoch 14/20\n","149/149 [==============================] - 74s 497ms/step - loss: 0.2691 - accuracy: 0.8881 - val_loss: 0.8869 - val_accuracy: 0.7626\n","Epoch 15/20\n","149/149 [==============================] - 72s 477ms/step - loss: 0.2799 - accuracy: 0.9003 - val_loss: 0.8073 - val_accuracy: 0.7626\n","Epoch 16/20\n","149/149 [==============================] - 71s 478ms/step - loss: 0.2784 - accuracy: 0.9070 - val_loss: 0.6513 - val_accuracy: 0.8283\n","Epoch 17/20\n","149/149 [==============================] - 82s 551ms/step - loss: 0.2096 - accuracy: 0.9245 - val_loss: 0.8211 - val_accuracy: 0.7677\n","Epoch 18/20\n","149/149 [==============================] - 72s 479ms/step - loss: 0.1654 - accuracy: 0.9447 - val_loss: 0.8178 - val_accuracy: 0.7828\n","Epoch 19/20\n","149/149 [==============================] - 72s 480ms/step - loss: 0.1780 - accuracy: 0.9340 - val_loss: 0.7666 - val_accuracy: 0.8232\n","Epoch 20/20\n","149/149 [==============================] - 74s 499ms/step - loss: 0.1894 - accuracy: 0.9245 - val_loss: 0.7963 - val_accuracy: 0.8081\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x271a86f01c0>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["classifier.fit_generator(\n","        generator=x_train,steps_per_epoch = len(x_train),\n","        epochs=20, validation_data=x_test,validation_steps = len(x_test))# No of images in test set"]},{"cell_type":"markdown","metadata":{"id":"r2EaBDWan4xm"},"source":["### Saving our model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56P8omD0n4xq"},"outputs":[],"source":["# Save the model\n","classifier.save('disaster.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epy7V5Yyn4xq"},"outputs":[],"source":["model_json = classifier.to_json()\n","with open(\"model-bw.json\", \"w\") as json_file:\n","    json_file.write(model_json)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEUnNEokn4xr"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"iIjvTKknn4xr"},"source":["### Predicting our results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxvhyIjmn4xs"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","from keras.preprocessing import image\n","model = load_model(\"disaster.h5\") #loading the model for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EnC_zAlYn4xt","outputId":"eca9347b-3e50-40e6-b2c8-a158ea6a4e91"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["img = image.load_img(r\"..\\dataset\\test_set\\Cyclone\\921.jpg\",grayscale=False,\n","                     target_size= (64,64))#loading of the image\n","x = image.img_to_array(img)#image to array\n","x = np.expand_dims(x,axis = 0)#changing the shape\n","pred = model.predict(x)#predicting the classes\n","pred = np.argmax(pred) #getting index of higher probability value\n","pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4UXL9Tpn4xu","outputId":"72419457-01e0-46d9-f838-b0e72582b147"},"outputs":[{"data":{"text/plain":["'Cyclone'"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["index=['Cyclone','Earthquake','Flood','Wildfire']\n","result=str(index[pred])\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfgM8Tvin4xw"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"AI based Natural disaster analysis.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}